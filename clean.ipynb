{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean.py  \n",
    "**Authors:** Arjun Sai Krishnan, Carina Lewandowski  \n",
    "**Description:** series of functions for preprocessing hCoV sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules from biopython\n",
    "from Bio import SeqIO, Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Extraneous Sequences\n",
    "This chunk removes sequences that contain non-real bases or do not meet the length threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# REMOVE EXTRANEOUS SEQUENCES \n",
    "# ---------------------------------------------------------------------------\n",
    "def remove_extr_seqs(records): \n",
    "    sequences = []\n",
    "    is_bad = False\n",
    "    count = 0\n",
    "    threshold = 29000 # length threshold\n",
    "\n",
    "    # iterate over each record & parse its sequence as a string\n",
    "    for record in records:\n",
    "        curr_seq = str(record.seq)\n",
    "        # trim white space for the purpose of inspecting base characters\n",
    "        curr_seq.replace(' ', '')\n",
    "        curr_seq.replace('\\n', '')\n",
    "        # check that seq length meets threshold\n",
    "        if len(curr_seq) < threshold:\n",
    "            is_bad = True\n",
    "        # visit each position to check for extraneous base; break and reomve\n",
    "        # sequence once one is found.\n",
    "        for char in curr_seq:\n",
    "            if char != 'A' and char != 'C' and char != 'T' and char !='G':\n",
    "                print(char)\n",
    "                count = count + 1\n",
    "                print(count)\n",
    "                is_bad = True\n",
    "                break\n",
    "        # append sequence to list if it is not extraneous\n",
    "        if not is_bad: \n",
    "            sequences.append(record)\n",
    "        # reset boolean check\n",
    "        is_bad = False\n",
    "        \n",
    "    # write non-extraneous sequences to a new text file in fasta format\n",
    "    SeqIO.write(sequences, \"cleaned.txt\", \"fasta\")\n",
    "    SeqIO.write(records[0], \"test.txt\", \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Gaps\n",
    "After alignment with MAFFT, gaps typically appear in the largest amounts at the beginning and ends of sequences. This chunk finds the right number of bases to trim from the beginning and ends of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# REMOVE GAPS\n",
    "# ---------------------------------------------------------------------------\n",
    "def remove_gaps(records):\n",
    "    # iterate over each record & parse its sequence as a string\n",
    "    for record in records:\n",
    "        curr_seq = str(record.seq)\n",
    "        # find length of consecutive gaps at the beginning of the seq\n",
    "        i = 0\n",
    "        max_gaps_beg = 0\n",
    "        while(curr_seq[i] == '-'):\n",
    "            i += 1\n",
    "        # find length of consecutive gaps at the end of the seq\n",
    "        j = -1\n",
    "        max_gaps_end = 0\n",
    "        while(curr_seq[j] == '-'):\n",
    "            j -= 1\n",
    "        # updated max lengths\n",
    "        if (i > max_gaps_beg): max_gaps_beg = i\n",
    "        if (-1*j > max_gaps_end): max_gaps_end = j\n",
    "\n",
    "    updated_records = []\n",
    "    # iterate over each record and trim front and back of sequences\n",
    "    for record in records:\n",
    "        new_record = record\n",
    "        new_seq = str(record.seq)[(max_gaps_beg - 1):(max_gaps_end + 1)]\n",
    "        new_seq = Seq.Seq(new_seq)\n",
    "        new_record.seq = new_seq\n",
    "        updated_records.append(new_record)\n",
    "\n",
    "    # write trimmed records to a new text file in fasta format\n",
    "    SeqIO.write(updated_records, \"aligned_cleaned.txt\", \"fasta\")\n",
    "    SeqIO.write(records[0], \"test_aligned.txt\", \"fasta\")\n",
    "    print(\"Trimmed from front: \", i)\n",
    "    print(\"Trimmed from back: \", j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Segregating Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# FIND SEGREGATING SITES\n",
    "# ---------------------------------------------------------------------------\n",
    "def find_seg_sites(records):\n",
    "    site_dict = {}\n",
    "    # iterate over each record & parse its sequence as a string\n",
    "    for record in records:\n",
    "        curr_seq = str(record.seq)\n",
    "        # iterate over each position in the current seq\n",
    "        for i in range(len(curr_seq)):\n",
    "            base = curr_seq[i]\n",
    "            # create a dict for each seq position\n",
    "            # (key: position, value: dict of bases)\n",
    "            if (i not in site_dict):\n",
    "                site_dict[i] = {}\n",
    "            # create a dict for each base found at each position\n",
    "            # (key: base, value: frequency)\n",
    "            if (base not in site_dict[i]):\n",
    "                site_dict[i][base] = 0    \n",
    "            site_dict[i][base] += 1\n",
    "    \n",
    "    # if more than one base dict exists at a position, classify as a seg site\n",
    "    seg_sites = []\n",
    "    for site in site_dict:\n",
    "        if (len(site_dict[site]) > 1):\n",
    "            seg_sites.append((site, site_dict[site]))\n",
    "\n",
    "    return seg_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TESTING find_seg_sites '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# TESTING\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "''' TESTING remove_extr_seqs '''\n",
    "# records = list(SeqIO.parse(\"gisaid_hcov-19_India_2020_11_11_15.txt\", \"fasta\"))\n",
    "# records = list(SeqIO.parse(\"test.txt\", \"fasta\"))\n",
    "\n",
    "''' TESTING remove_gaps '''\n",
    "# records2 = list(SeqIO.parse(\"aligned.txt\", \"fasta\"))\n",
    "# remove_gaps(records2)\n",
    "\n",
    "''' TESTING find_seg_sites '''\n",
    "# records3 = list(SeqIO.parse(\"aligned_cleaned.txt\", \"fasta\"))\n",
    "# seg_sites = find_seg_sites(records3)\n",
    "# print(\"Number of segregating sites:\", len(seg_sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
